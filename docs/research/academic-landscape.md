# Academic Landscape  
A survey of research areas relevant to Symbia’s cognitive‑architecture execution layer.

---

## 1. Cognitive Architectures (Explicit Models of Mind)

### **Global Workspace Theory (GWT) — Baars, Dehaene**
Focus: consciousness as a broadcast workspace.  
Relevance: Symbia’s continuity + cross‑agent coordination mirrors global broadcasting of state.

### **LIDA / IDA — Franklin, Baars**
Focus: perception–memory–action loop with explicit attention cycles.  
Relevance: Direct mapping to Symbia’s cycle: observe → integrate → update identity graph → plan.

### **ACT‑R — Anderson**
Focus: modular cognition with declarative + procedural memory.  
Relevance: Clean separation of long‑term memory vs. working memory informs Symbia’s state substrate.

### **Soar — Laird**
Focus: problem‑solving with chunking for long‑term learning.  
Relevance: Guides how Symbia can accumulate stable patterns across time without retraining a model.

---

## 2. Long‑Horizon Reasoning & Planning

### **Hierarchical Reinforcement Learning**
(Feudal RL; Options framework)  
Relevance: Delegated cognitive routines → analog to multi‑agent sub‑tasks within Symbia.

### **Model‑Based RL & World Models — Ha, Schmidhuber**
Relevance: The “learn a model of the world” approach validates Symbia’s identity graph as an evolving internal model.

---

## 3. Memory Systems for AI

### **Neural Turing Machines / Differentiable Neural Computers — DeepMind**
Relevance: Architectural inspiration for externalized memory but lacks semantics or governance.

### **Memory‑Augmented Transformers**
Relevance: Confirms that architectural scaffolding, not scale, is the bottleneck.

### **Retrieval‑Augmented Generation (RAG)**
Relevance: Proof that external memory works, but limited by lack of continuity or identity.

---

## 4. Multi‑Agent Research

### **MAS from Distributed AI**
Relevance: Traditional multi‑agent systems define protocols, identity, and coordination — but assumed symbolic agents, not LLM‑based ones.

### **OpenAI’s “Self‑Play” line of work**
Relevance: Agents can learn coordination but still lack persistent identity/state.

### **Anthropic’s constitutional models**
Relevance: Shows how constraints can be encoded, but not how they persist across time.

---

## 5. Cognitive Science & Developmental Psychology  
**Piaget, Vygotsky** — staged learning & scaffolding  
Relevance: Symbia’s role: cognitive scaffolding layer above LLMs.  

**Theory of Mind research**  
Relevance: Symbia provides ToM continuity that LLMs cannot hold across resets.

---

## 6. Human–Computer Interaction (HCI)  
**Activity Theory, Distributed Cognition, Situated Action**  
Relevance: Humans think in context; Symbia preserves context across time.

---

## 7. Open Questions the Field Hasn’t Solved  
- How to maintain identity in stateless models  
- How to enforce constraints over long horizons  
- How to support multi‑agent collaboration with persistent memory  
- How to represent user cognition faithfully + ethically  
- How to measure continuity and trust

---

## 8. Summary  
No major research effort has produced a full architecture for durable, identity‑bound, constrained, continuity‑preserving cognition using LLMs.  
Symbia occupies the gap between cognitive architectures and real‑world AI engineering — unclaimed in both.

